# ğŸ¤Ÿ DocumentaciÃ³n Educativa: Reconocimiento de Lengua de SeÃ±as

## Tabla de Contenidos
1. [IntroducciÃ³n](#introducciÃ³n)
2. [Conceptos Fundamentales](#conceptos-fundamentales)
3. [Arquitectura General del Sistema](#arquitectura-general-del-sistema)
4. [Paso 1: Captura de Datos](#paso-1-captura-de-datos)
5. [Paso 2: Entrenamiento del Modelo](#paso-2-entrenamiento-del-modelo)
6. [Paso 3: PredicciÃ³n en Tiempo Real](#paso-3-predicciÃ³n-en-tiempo-real)
7. [TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)

---

## IntroducciÃ³n

Este proyecto es un **sistema de reconocimiento de lengua de seÃ±as** en tiempo real. Combina dos tecnologÃ­as poderosas:

- **MediaPipe Holistic:** Detecta puntos de referencia en las manos, rostro y cuerpo.
- **Redes Neuronales LSTM:** Aprende patrones de movimiento y postura a partir de esos puntos.

El objetivo es que la computadora pueda "entender" y clasificar diferentes seÃ±as (letras, nÃºmeros, palabras) capturadas en video.

---

## Conceptos Fundamentales

### 1. Â¿QuÃ© es MediaPipe Holistic?

MediaPipe es una biblioteca de visiÃ³n por computadora desarrollada por Google que puede detectar:

- **33 puntos de referencia en el rostro** (nariz, ojos, boca, etc.)
- **21 puntos en cada mano** (dedos, palma, articulaciones)
- **17 puntos en el cuerpo** (hombros, caderas, rodillas, etc.)

Estos puntos se representan con coordenadas (x, y, z) que indican posiciÃ³n en el espacio.

```
Ejemplo de un punto: (0.45, 0.32, -0.12)
- x: posiciÃ³n horizontal (0 a 1, siendo 0 izquierda y 1 derecha)
- y: posiciÃ³n vertical (0 a 1, siendo 0 arriba y 1 abajo)
- z: profundidad relativa a la cÃ¡mara (valores negativos = mÃ¡s cerca)
```

### 2. Â¿QuÃ© son los Landmarks?

Un "landmark" es un punto detectado. En este proyecto:
- Recolectamos **147 valores** por frame (caracterÃ­sticas):
  - 63 valores de la mano izquierda (21 puntos Ã— 3 coordenadas)
  - 63 valores de la mano derecha (21 puntos Ã— 3 coordenadas)
  - 6 valores de los hombros (2 puntos Ã— 3 coordenadas)
  - 15 valores del rostro (5 puntos Ã— 3 coordenadas)

### 3. Â¿QuÃ© es una Secuencia?

Una **secuencia** es una colecciÃ³n de frames consecutivos:
- **30 frames** por secuencia (aproximadamente 1 segundo a 30 FPS)
- Cada frame contiene 147 valores de landmarks
- Forma: **(30, 147)**

Esto permite que el modelo capture **tanto postura como movimiento**.

### 4. NormalizaciÃ³n de Landmarks

Antes de usar los landmarks, los normalizamos para hacerlos **independientes de posiciÃ³n y tamaÃ±o**:

1. **Centrar:** Restamos el promedio de todos los puntos, centrando la seÃ±a en el origen.
2. **Escalar:** Dividimos por la distancia mÃ¡xima, haciendo que todas las seÃ±as tengan un "tamaÃ±o" similar.

Esto permite que el modelo reconozca la seÃ±a aunque la persona estÃ© mÃ¡s o menos cerca de la cÃ¡mara.

---

## Arquitectura General del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CAPTURA     â”‚
â”‚  (CÃ¡mara Web)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Video en vivo
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. DETECCIÃ“N MediaPipe     â”‚
â”‚  (33 puntos cara,           â”‚
â”‚   21 puntos por mano,       â”‚
â”‚   17 puntos cuerpo)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 147 valores (landmarks)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. NORMALIZACIÃ“N           â”‚
â”‚  (Centrar, escalar)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Secuencia (30, 147)
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Â¿Entrenando?  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
         â”‚        â”‚
         SI      NO
         â”‚        â”‚
         â–¼        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GUARDARâ”‚  â”‚ 4. MODELO LSTM         â”‚
    â”‚ .npy   â”‚  â”‚ (PredicciÃ³n)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Clase predicha         â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Paso 1: Captura de Datos

### Â¿Por quÃ© necesitamos capturar datos?

Las redes neuronales aprenden **observando ejemplos**. Necesitamos mÃºltiples grabaciones de cada seÃ±a para que el modelo aprenda sus caracterÃ­sticas.

### Â¿CÃ³mo funciona la captura?

**Script:** `capturar_secuencias.py`

1. **Usuario ingresa el nombre de la seÃ±a** (ej: "hola")
2. **El programa abre la cÃ¡mara** y muestra vista previa
3. **Usuario presiona 'C'** para comenzar a grabar
4. **Se capturan 30 frames** (1 segundo) de landmarks
5. **Se repite 30 veces** para la misma seÃ±a (30 secuencias)
6. **Se genera un GIF** mostrando la seÃ±a capturada

### Archivo de salida

Cada secuencia se guarda como:
```
data/secuencias/hola/hola_0.npy    (Secuencia 1)
data/secuencias/hola/hola_1.npy    (Secuencia 2)
...
data/secuencias/hola/hola_29.npy   (Secuencia 30)
```

Formato: Array NumPy con forma **(30, 147)**

### CÃ³digo clave

```python
# En capturar_secuencias.py
for i in range(Config.SEQUENCES_PER_CLASS):  # 30 secuencias
    while len(secuencia) < Config.FRAMES_PER_SEQUENCE:  # 30 frames
        ret, frame = cap.read()
        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        landmarks = extract_holistic_landmarks(results)  # 147 valores
        landmarks = normalize_landmarks(landmarks)
        
        if validate_landmarks(landmarks):  # Validar que sean buenos
            secuencia.append(landmarks)
    
    np.save(f"data/secuencias/hola/hola_{i}.npy", secuencia)
```

---

## Paso 2: Entrenamiento del Modelo

### Â¿QuÃ© es el entrenamiento?

El entrenamiento es el proceso donde la red neuronal **aprende a reconocer patrones** a partir de los datos capturados.

**Script:** `entrenar_modelo.py`

### Fases del entrenamiento

#### 2.1 Carga de Datos

```python
def cargar_datos(augment=False):
    X, y = [], []
    # X: lista de secuencias (datos de entrada)
    # y: lista de etiquetas (quÃ© seÃ±a es cada una)
    
    for clase in ['hola', 'adios', 'gracias']:
        for archivo in os.listdir(f'data/secuencias/{clase}'):
            secuencia = np.load(archivo)  # Forma: (30, 147)
            X.append(secuencia)
            y.append(indice_clase)
```

#### 2.2 Aumento de Datos

Para mejorar la precisiÃ³n, generamos **variaciones de cada secuencia**:
- RotaciÃ³n aleatoria (Â±5 grados)
- Ruido gaussiano (pequeÃ±as perturbaciones)

Esto multiplica el dataset sin necesidad de grabar mÃ¡s.

```python
def augment_sequence(sequence):
    # Aplica rotaciÃ³n 3D
    angle = random.uniform(-5, 5)
    rotation_matrix = crear_matriz_rotacion(angle)
    
    # Aplica ruido
    noise = np.random.normal(0, 0.005, secuencia.shape)
    
    return secuencia_rotada + noise
```

**Resultado:** De 300 secuencias â†’ 900 secuencias (triplicamos el dataset)

#### 2.3 DivisiÃ³n Train/Validation

```python
X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,  # 80% entrenamiento, 20% validaciÃ³n
    random_state=42,
    stratify=y  # Mantener proporciÃ³n de clases
)
```

#### 2.4 Modelo LSTM

**Â¿QuÃ© es LSTM?**

LSTM = **Long Short-Term Memory** (Memoria a Largo y Corto Plazo)

Es una red neuronal recurrente especial que:
- Procesa **secuencias** de datos (perfecta para video)
- Recuerda informaciÃ³n importante del pasado
- Olvida informaciÃ³n irrelevante
- Realiza buenas predicciones basÃ¡ndose en patrones temporales

**Arquitectura del modelo:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Entrada: Secuencia (30, 147)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LSTM (128 neuronas)â”‚ â† Lee la secuencia
        â”‚ return_sequences   â”‚   completa
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ BatchNormalizationâ”‚ â† Normaliza salida
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dropout (30%)      â”‚ â† RegularizaciÃ³n
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LSTM (64 neuronas) â”‚ â† Extrae caracterÃ­sticas
        â”‚ return_sequences   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ BatchNormalizationâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dropout (30%)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LSTM (32 neuronas) â”‚ â† SÃ­ntesis final
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ BatchNormalizationâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dense (64, ReLU)       â”‚ â† Procesamiento denso
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dense (N_clases, Softmax)  â”‚ â† Probabilidades
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Salida: PredicciÃ³n       â”‚
        â”‚ Ej: [0.02, 0.95, 0.03]  â”‚
        â”‚ Clase predicha: HOLA     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Â¿Por quÃ© 3 capas LSTM?**
- **Capa 1 (128):** Aprende patrones bÃ¡sicos
- **Capa 2 (64):** Combina patrones en conceptos mÃ¡s complejos
- **Capa 3 (32):** SÃ­ntesis final para clasificaciÃ³n

#### 2.5 CompilaciÃ³n y Entrenamiento

```python
modelo.compile(
    loss='sparse_categorical_crossentropy',  # FunciÃ³n de pÃ©rdida
    optimizer='adam',                        # Optimizador
    metrics=['accuracy']                     # MÃ©trica
)

modelo.fit(
    X_train, y_train,
    epochs=150,              # MÃ¡ximo 150 iteraciones
    validation_data=(X_val, y_val),
    callbacks=[early_stop, reduce_lr],
    batch_size=32
)
```

**Â¿QuÃ© es cada componente?**

- **Loss (PÃ©rdida):** Mide quÃ© tan mal predice el modelo. Objetivo: reducirlo.
- **Optimizer (Adam):** Algoritmo que ajusta los pesos del modelo para reducir la pÃ©rdida.
- **Accuracy:** Porcentaje de predicciones correctas.
- **Epochs:** NÃºmero de veces que el modelo ve todo el dataset.
- **Batch Size:** CuÃ¡ntas secuencias procesa antes de actualizar pesos.

**Callbacks (Funciones especiales):**
- **EarlyStopping:** Detiene el entrenamiento si no mejora (evita sobreentrenamiento).
- **ReduceLROnPlateau:** Reduce la velocidad de aprendizaje si se estanca.

#### 2.6 Guardar Modelo

```python
modelo.save(Config.MODEL_PATH)  # Guarda en models/modelo_lstm.h5
pickle.dump(etiquetas, open(Config.LABELS_PATH, 'wb'))  # Guarda las clases
```

---

## Paso 3: PredicciÃ³n en Tiempo Real

### Â¿CÃ³mo funciona la predicciÃ³n?

**Script:** `predecir_secuencias.py`

Una vez que tenemos el modelo entrenado, lo usamos para **clasificar seÃ±as nuevas** en tiempo real.

### Flujo de PredicciÃ³n

```
1. Capturar frame actual
         â”‚
         â–¼
2. Detectar landmarks (MediaPipe)
         â”‚
         â–¼
3. Normalizar landmarks
         â”‚
         â–¼
4. Agregar a buffer de 30 frames
         â”‚
         â–¼
5. Buffer lleno (30 frames) â†’
         â”‚
         â–¼
6. Pasar a modelo LSTM
         â”‚
         â–¼
7. Obtener predicciÃ³n (vector de probabilidades)
         â”‚
         â–¼
8. Verificar confianza y consistencia
         â”‚
         â”œâ”€ Confianza < 80% â†’ Descartar
         â”‚
         â”œâ”€ Brecha entre clases < 15% â†’ Descartar (ambiguo)
         â”‚
         â””â”€ Todo bien â†’ Mostrar resultado
```

### CÃ³digo clave

```python
# Buffer para almacenar 30 frames consecutivos
buffer = deque(maxlen=30)

while True:
    ret, frame = cap.read()
    results = holistic.process(frame)
    landmarks = extract_holistic_landmarks(results)
    landmarks = normalize_landmarks(landmarks)
    
    if validate_landmarks(landmarks):
        buffer.append(landmarks)
    
    # Cuando tenemos 30 frames
    if len(buffer) == 30:
        secuencia = np.array(buffer).reshape(1, 30, 147)
        pred = model.predict(secuencia)[0]
        
        # Obtener top 2 predicciones
        idx_mejor = np.argmax(pred)
        confianza = pred[idx_mejor]
        idx_segundo = np.argmax(pred[pred < confianza])
        confianza_segundo = pred[idx_segundo]
        
        brecha = confianza - confianza_segundo
        
        # Verificar calidad de predicciÃ³n
        if (confianza >= 0.80 and
            brecha >= 0.15 and
            predicciones_consistentes >= 3):
            
            clase_predicha = etiquetas[idx_mejor]
            print(f"ğŸ¯ {clase_predicha.upper()}")
            
            # Mostrar GIF de referencia
            mostrar_gif(clase_predicha)
        
        buffer.clear()
```

### MÃ©tricas de Confianza

Para evitar falsos positivos, verificamos:

1. **MIN_CONFIDENCE = 0.80** (80%)
   - El modelo debe estar al menos 80% seguro de su predicciÃ³n

2. **MIN_CONFIDENCE_GAP = 0.15** (15%)
   - Diferencia entre la clase mÃ¡s probable y la segunda mÃ¡s probable
   - Evita confusiones entre clases similares (ej: A y E)

3. **CONSISTENT_PREDICTIONS = 3**
   - Debe repetir la misma predicciÃ³n al menos 3 veces
   - Evita falsos positivos de un solo frame

**Ejemplo:**

```
PredicciÃ³n: [0.05, 0.82, 0.13]
    â†“
Clase mejor: "HOLA" (82%)
Clase segundo: "A" (13%)
Brecha: 82% - 13% = 69% âœ“ (> 15%)
Confianza: 82% âœ“ (> 80%)
Consistencia: 3/3 veces âœ“

âœ… PREDICCIÃ“N VÃLIDA: HOLA
```

---

## TecnologÃ­as Utilizadas

### 1. **MediaPipe**
- Detecta puntos de referencia en tiempo real
- Modelo preentrenado en millones de imÃ¡genes
- PrecisiÃ³n: ~95%

### 2. **TensorFlow/Keras**
- Framework para redes neuronales
- Proporciona capas LSTM, Dense, BatchNormalization
- Optimizadores (Adam)

### 3. **OpenCV (cv2)**
- Captura de video desde cÃ¡mara web
- ManipulaciÃ³n de imÃ¡genes
- VisualizaciÃ³n en tiempo real

### 4. **NumPy**
- Operaciones numÃ©ricas eficientes
- Manejo de arrays multidimensionales
- Transformaciones matemÃ¡ticas (rotaciÃ³n, escala)

### 5. **Scikit-learn**
- DivisiÃ³n train/test
- Preprocesamiento de datos

### 6. **Pillow (PIL)**
- GeneraciÃ³n de GIFs
- Procesamiento de imÃ¡genes

---

## Resumen de Flujo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INICIO                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 1. CAPTURAR DATOS                  â”‚
        â”‚    (capturar_secuencias.py)        â”‚
        â”‚    Crea: data/secuencias/*/        â”‚
        â”‚    Genera: gifs/*.gif              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 2. ENTRENAR MODELO                 â”‚
        â”‚    (entrenar_modelo.py)            â”‚
        â”‚    Lee: data/secuencias/*/         â”‚
        â”‚    Crea: models/modelo_lstm.h5     â”‚
        â”‚    Crea: models/etiquetas.pkl      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 3. PREDECIR EN TIEMPO REAL         â”‚
        â”‚    (predecir_secuencias.py)        â”‚
        â”‚    Lee: models/modelo_lstm.h5      â”‚
        â”‚    Lee: gifs/*.gif                 â”‚
        â”‚    Salida: Predicciones en pantallaâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           RESULTADO FINAL            â”‚
        â”‚  Reconocimiento de Lengua de SeÃ±as  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mejoras y Extensiones Futuras

1. **Aumentar Dataset:** Capturar mÃ¡s secuencias por clase
2. **Agregar mÃ¡s clases:** Palabras complejas, frases completas
3. **Transfer Learning:** Usar modelos preentrenados
4. **Interfaz GrÃ¡fica:** Crear GUI amigable
5. **Exportar a Mobile:** Llevar el modelo a dispositivos mÃ³viles
6. **Reconocimiento de Contexto:** Secuencias de mÃºltiples palabras

---

**Autores:** Harold Samuel Moreno RamÃ­rez | Diego MartÃ­nez Benites

**Ãšltima actualizaciÃ³n:** Noviembre 2025
